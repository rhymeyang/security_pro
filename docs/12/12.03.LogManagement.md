---
lang: zh-CN
title: Section 12.3 Log Management
description: some description
---

As you study this section, answer the following questions:

<ul>
    <li>
     What does a security information and event management (SIEM) system do?
    </li>
    <li>
     Why are trends important for network management?
    </li>
    <li>
     What part does event correlation play in a SIEM?
    </li>
    <li>
     How do IT security teams use alerts?
    </li>
</ul>

In this section, you will learn to:

<ul>
    <li>
     Use vulnerability scan outputs as part of SIEM.
    </li>
    <li>
     Identify trends and use them appropriately.
    </li>
    <li>
     Identify uses for SIEM.
    </li>
</ul>

Key terms for this section include the following:

<table class="terms">
   <thead>
    <tr><th class_="firstTableHeader" scope="col" class="fw-bold">
     Term
    </th>
    <th scope="col" class="fw-bold">
     Definition
    </th>
   </tr></thead>
   <tbody><tr>
    <td>
     SIEM
    </td>
    <td>
     A software tool used to compile and examine multiple data points gathered from across a network.
    </td>
   </tr>
   <tr>
    <td>
     Sensor
    </td>
    <td>
     A devise that gathers data from a device or system. It provides the collected data to a monitoring
        system.
    </td>
   </tr>
   <tr>
    <td>
     Trend
    </td>
    <td>
     Patterns of activity discovered and reported to the SIEM.
    </td>
   </tr>
   <tr>
    <td>
     Sensitivity
    </td>
    <td>
     Customized threshold for sensor data that is sent to the SIEM.
    </td>
   </tr>
</tbody></table>

This section helps you prepare for the following certification exam objectives:

<table class="objectives">
   <thead>
    <tr><th class_="firstTableHeader" scope="col" class="fw-bold">
     Exam
    </th>
    <th scope="col" class="fw-bold">
     Objective
    </th>
   </tr></thead>
   <tbody><tr>
    <td>
     TestOut Security Pro
    </td>
    <td>
     5.1 Implement Logging and Auditing Implement Logging and Auditing
     <blockquote>
      Configure Advanced Audit Policy
      <br>
      Enable Device Logs
     </blockquote>
    </td>
   </tr>
   <tr>
    <td>
     Comptia Security+
    </td>
    <td>
     4.3. Given an incident, utilize appropriate data sources to support an investigation
     <ul>
      <li>
       Vulnerability scan output
      </li>
      <li>
       SIEM dashboards
       <ul>
        <li>
         Sensor
        </li>
        <li>
         Sensitivity
        </li>
        <li>
         Trends
        </li>
        <li>
         Alerts
        </li>
        <li>
         Correlation
        </li>
       </ul>
      </li>
     </ul>
    </td>
   </tr>
</tbody></table>

## 12.3.1 Security Information and Event Management

### SIEM 00:00-00:17

In this video, we'll discuss Security Information and Event Management, or SIEM, which is a tool used to compile and examine multiple data points gathered from across a network. We'll also explore log management.

### Vulnerability Scan Output 00:17-00:43

Monitoring a network requires experience and solid tools. One common network security tool is a scanner that can identify vulnerabilities and recommend remediation steps. The scan delivers output information to the IT administrators via the SIEM dashboard. The interval between scans is set by the IT department. This tool also scans servers, firewalls, switches, software programs, and even security cameras and wireless access points.

### SIEM Dashboards 00:43-01:04

The are many versions of SIEMs. Each one has different features and benefits, but all SIEMs have several features in common. One of these is the dashboard. These are generally customizable information screens that show real-time security and network information. This allows the IT security team to monitor events as they occur on the network.

### Sensors 01:04-01:19

Sensors play a vital role in monitoring and securing a network. These sensors are set up at critical endpoints, services, and other vulnerable locations. They're programmed to send customized alerts to the SEIM if certain parameters are reached or exceeded.

### Sensitivity 01:19-01:37

Before sensors are deployed, the IT security team sets their sensitivity. The benefit to variable sensitivity settings is the ability to customize the data that's sent to the SIEM. Not every company has the same needs, and that's what makes sensor-sensitivity customization so beneficial.

### Trends 01:37-02:03

Trends are patterns of activity discovered and reported to the SEIM. This is how baselines are established. These trends help security analysts decide if reported activity is normal or outside of the baseline. Trends that don't fit previously recorded ones can be investigated. As the IT security team investigates and documents these trends, it becomes easier for them to quickly spot an anomaly that may signal a security concern.

### Alerts 02:03-02:28

Alerts are the SIEMs' way of letting the IT team know that a pre-established parameter has been contravened. The alert is intended to get the attention of the IT person who's monitoring the network. A best practice in this area is 24-hour monitoring. This means that weekends, holidays, and early hours are all filled. Hackers don't keep normal hours, and network equipment can break at the most inconvenient times!

### Correlation 02:28-02:54

Event correlation is a critical part of using a SIEM solution. The software gathers data from log files, system applications, network appliances, and other endpoints in order to analyze it. This work is tedious, and people are inefficient at it. That's why the event-correlation feature is valuable. Not only does it gather data, but it analyzes and compares known malicious behavior against the aggregate data so that events aren't missed.

### Summary 02:54-03:35

That's it for this lesson. In this lesson, we learned about SIEMs and the features that make them a critical network component. These include customizable sensors and their placement. Trends help IT teams to establish baselines or norms for their network. Alerts provide critical information to the network monitor so that appropriate action can be taken. Event correlation automates a very laborious process and analyzes and compares aggregate data to known security behaviors and events, ensuring that nothing gets by undetected. All this is delivered to the IT team via a customizable dashboard for easy examination.

## 12.3.2 Log Management

### Log Management 00:00-00:12

In this video, we'll explore log categories, specific logs you should know, and open-source tools that are used to aid IT security teams.

### Network Logs 00:12-00:29

Every network generates dozens and dozens of logs. Network logs tell us what's coming into and leaving our network. Every network appliance and almost every application produces logs which can be used for a variety of reasons. But more often than not, we use logs for network security.

### System Logs 00:29-00:51

System logs are produced by an operating system. These logs contain all the information that pertains to that OS like updates, errors, failures, and other system occurrences. This includes information for client computers and servers, which gives IT admins a way to investigate events on individual machines that may interconnect with other machines on the network.

### Application Logs 00:51-01:12

Most applications produce some type of event logging. These logs show application access, crashes, updates, and any other relevant information which could be valuable in determining root-cause analysis. The application may be crashing or not performing correctly, and this could be tied to suspicious activity that may indicate malicious intent.

### Security Logs 01:12-01:34

There are several logs that would fall under the security category. There are application-security logs, event-security logs, and security logs for specialty applications like IDS/IPS, endpoints, firewalls, routers, and switches. Also, logs for security cameras and wireless or physical access points are included under the security log umbrella.

### Web Server Logs 01:34-02:03

There's no doubt that web server logs are one of the most tedious of all logs to parse. But web servers can be prime targets for hackers, so it's important to know who's interacting with your server and what they're attempting to do. Most web engines like IIS, Tomcat, Web Sphere, and NGINX have some level of server logging. These logs can tell you exactly when users log onto your site and what their location is. They also give you some information on attempted attacks.

### DNS 02:03-02:43

Domain Name System, or DNS, has been around for a long time. When DNS was designed, network security wasn't a priority. Over time, malicious actors started using DNS-targeted attacks. These attacks have the potential to be disruptive and quite expensive. With DNS logging, you can track updates and choose to approve or deny them. DNS also produces query logs that detail which requests are being handled by which instance. Rate limiting is another valuable tool that limits response rate. Analyze these logs to see when rate limiting was used and for what purpose. Client IP, record requests, flags, and other metadata can be included in these logs.

### Dump Files 02:43-03:19

Dump files are created when an application, OS, or other computer function stops abruptly. The information that's stored in memory at the time is dropped into a file for later analysis. These files help IT admins perform root-cause analysis because they provide the state that the application was in when it crashed, error codes, and other clues as to what happened previous to the application failure. They can also give clues as to the crash's origin. This could be something as commonplace as a bad driver or hardware component, or it may prove, unfortunately, to be the result of a malicious act.

### Authentication Logs 03:19-03:41

Authentication logs are vital to a network's security. Authentication servers may be Active Directory-based or OpenLDAP depending on your network structure. It's critical to know who may be poking around your network, so token requests, authentication failures, or failed logins on expired accounts are all stored on these authentication logs for you to view.

### VoIP 03:41-04:01

Voice over Internet Protocol, or VoIP, has become a common network application. With a high implementation rate comes attention from malicious actors. As with any network application, there are vulnerabilities that can be leveraged, so in order to defend it there needs to be a way to access information about what's happening at any given time.

### SIP Traffic 04:01-04:22

Session Initial Protocol, or SIP, is the standard in VoIP calls. The key to tracking attacks against a VoIP system is understanding SIP and being able to parse its logs. These logs contain key information about where a call was initiated and what the communication's intent was. These facts help the IT security team create a stronger SIP security posture.

### Syslog/Rsyslog/Syslog-ng 04:22-05:25

Syslog is short for System Logging Protocol. This protocol sends system logs and event messages to a server designated by the system administrator. It collects logs from various appliances and sends them to the syslog server where they can be reviewed and analyzed.

Rsyslog is an open-source tool created for use in Linux networks. It stands for rocket-fast system-log processing. It gets its name because of its ability to send a million messages per second to a local server. This tool's benefit is its diversity. It's capable of multi-threading by leveraging multiple security protocols like TCP, TLS, and others. It also allows for output-format customization.

Syslog-ng is a robust log-aggregating software for multiple platforms, including Windows. This tool increases the quality of the log data that's sent to your SIEM. It also facilitates lightning-fast log searches by using full-text queries and collects logs without the installation of server agents.

### Journalctl 05:25-06:03

Journactl is a Linux tool that gathers the logs produced by systemd, a system that's the basis for many Linux components. This command is used in Bash to parse logs that've been collected by systemd. The results are presented in the syslog format and are ordered oldest to newest by default. This can be changed by using the –r flag. Each line shows the date, server hostname, process name, and any messages. If you're more comfortable using a command line tool, this is for you. Because it's a CLI tool, there are many key commands at your disposal to get you your logs quickly.

### Nxlog 06:03-06:23

Nxlog is an open-source log-collector application. It uses log-collector agents to gather and send log data to a log server, which can itself be set up using nxlog. This application is available for both Windows and Linux. The Community edition supports multiple SIEM applications and works with Windows Event Viewer and syslog.

### Summary 06:23-06:51

That's it for this lesson. In this lesson, we learned about different log categories like network logs, security logs, and application logs. We also learned about specific logs and how you can use them for better security. Finally, we learned about open-source tools that assist you in collecting and organizing log data and metadata. Understanding logs is truly the key to better security.

## 12.3.3 SIEM and Log Management Facts

A security information and event management (SIEM) system combines security information management (SIM) and security event management (SEM) functions into one security management system.

This lesson covers the topic of security information and event management.

### Security Information and Event Management

Security information and event management tools compile and examine multiple data points gathered from across a network. The following table describes SIEM components.

<table>
   <thead>
    <tr><th class_="firstTableHeader" scope="col" class="fw-bold">
     Component
    </th>
    <th scope="col" class="fw-bold">
     Description
    </th>
   </tr></thead>
   <tbody><tr>
    <td>
     Vulnerability scan output
    </td>
    <td>
     Monitoring a network requires experience and solid tools. One tool common to network security is a
        scanner that can identify vulnerabilities and recommend remediation steps. This tool scans servers,
        firewalls, switches, software programs, security cameras, and wireless access points. The scan delivers the output to IT admins via the SIEM dashboard. The interval between scans is set by the IT department.
    </td>
   </tr>
   <tr>
    <td>
     SIEM dashboards
    </td>
    <td>
     The dashboard is a common component of all SIEM systems. The dashboard consists of customizable information screens that show
        real-time security and network information. The information in real time allows the IT security team to effectively monitor and respond to events on the network.
    </td>
   </tr>
   <tr>
    <td>
     Sensors
    </td>
    <td>
     Sensors are a vital part of monitoring and securing a network. Sensors are set-up at critical
        endpoints, services, and other vulnerable locations. These sensors are programmed to send customized alerts to the SEIM if certain parameters are not within the acceptable range.
    </td>
   </tr>
   <tr>
    <td>
     Sensitivity
    </td>
    <td>
     When the sensors are deployed, the sensitivity level is set by the IT security team. The benefit of variable
        sensitivity settings is the ability to customize the data that is sent to the SIEM. Not every organization will have the same
        needs in network monitoring.
    </td>
   </tr>
   <tr>
    <td>
     Trends
    </td>
    <td>
     Trends are patterns of activity discovered and reported to the SIEM. This is how baselines are
        established. Trends help security analysts decide if reported activity is normal or outside of the baseline. Trends that do not fit previously recorded information can be investigated by the security group. As the IT security team investigates and documents these trends it becomes easier for the team to quickly spot a trend that may signal a security
        event.
    </td>
   </tr>
   <tr>
    <td>
     Alerts
    </td>
    <td>
     Alerts are the SIEM’s way of letting the IT team know that a pre-established parameter is not within the acceptable range. The alert is intended to get the attention of the IT person, or persons, monitoring the network. A best
        practice in this area is 24-hour monitoring.
    </td>
   </tr>
   <tr>
    <td>
     Correlation
    </td>
    <td>
     Event correlation is a critical SIEM component. The software gathers data from log files,
        system applications, network appliances, etc., and analyzes it. This work is tedious; people are
        inefficient at it. That’s why the event correlation feature is valuable. Not only does it gather the data, but it
        analyzes and compares known malicious behavior against the aggregated data, increasing the chances of the discovery of security events.
    </td>
   </tr>
</tbody></table>

## 12.3.4 Monitoring Data and Metadata

### Monitoring Data and Metadata 00:00-00:20

In this lesson, we'll cover the use of bandwidth monitors and metadata from emails, mobile devices, web traffic, and files. We'll also talk about using netflow/sflow echo and Ipfix. Let's get started.

### Bandwidth Monitors 00:20-01:44

Bandwidth monitors are a type of application that help network admins understand bandwidth usage. The first order of business with these applications is to establish a baseline. The longer a monitor runs, the more data points are created. When a substantial number of data points are created, a normal usage becomes apparent. Normal bandwidth usage is relative and varies by hours in the day, days of the week, and even weeks within the year. A baseline provides something concrete to compare against current usage or even suspect bandwidth usage.

There are many bandwidth monitoring applications available. There are cloud-based apps, on-premise apps, open source, and paid. Each one looks and functions differently, but each has the same goal, which is to allow easy access to bandwidth monitoring. The key is to learn how to establish a monitoring schedule and how to use the dashboard.

Here, we have some screenshots of different pages within a dashboard. As you can see, this graphic is showing bandwidth usage be the hour and below usage per day aggregated over time. This is the baseline that suspect usage is compared against. The second graphic shows the last hour's usage and the usage for a user-specified time frame. This allows for deeper examination of suspect bandwidth usage.

### Email Metadata 01:44-02:18

Email is a great tool that almost everyone uses to communicate. It's also the avenue for the majority of malicious network breaches. Fortunately, email provides metadata so it can be traced. All emails come with a header that contains information about both the sender and recipient. Parts of the headers can be spoofed to give investigators false information. The good news is that there are security devices that put X-headers throughout an emails' headers. These provide the originating email account and IP address, not the spoofed one.

### Mobile Metadata 02:18-02:52

Communications sent via mobile devices are common today. They come from tablets, laptops, smart phones, smart watches, and any other portable device that connects to the internet. These devices send emails and text messages and use apps to allow data and photo sharing. All this data produces metadata that can be used to identify people, places, times, and even deleted data. Pictures can be time stamped and geolocation stamped. Much of this metadata also reveals the origination of the data and the sender.

### Web Metadata 02:52-03:35

Websites produce many types of metadata. In fact, the metadata on a user's machine versus the server can be very different. The data on the opposite side of the transmission can help fill in gaps and corroborate findings on the opposing machine. Metadata includes IP addresses, user requests, user downloads, time spent on the site, and even attempts to gain unauthorized access. Web metadata also includes cookies, browser history, and even cached pages. Many times, malicious actors will attempt to obfuscate their real metadata. But the good news is that there are ways of finding the real metadata, especially for trained forensic investigators.

### File Metadata 03:35-04:25

Files produce many types of metadata. The first kind is File Creation (date/time). The File Creation data is the first time the file was written to the storage media it's currently on. This means that the file can be created in a different place and then moved or copied to a new location.

Last written refers to the last time a file was saved for any reason. It could prove that the file was copied from a different location or that the file was altered. Last access is any time a file is touched. When combined with file creation metadata on a user's machine, it can establish the probability that the file was copied from one machine to another. It also introduces the idea that a third device is involved in data copy process. Once the third device is found, it makes obtaining answers much easier.

### Netflow/sFlow 04:25-05:01

Network admins are always looking for a way to look at what's happening inside their network. Netflow is a session sampling protocol. This protocol works at Layers 2 through 4. It can exam each data flow that comes through or be set to sample sessions at certain intervals. If you want to sample packets in a broader layer range, then sFlow is your answer. This protocol works on Layers 2 through 7. Unlike Netflow, sFlow examines packets and can only be used in sampling mode. This is stateless packet sampling that provides information efficiently.

### Ipfix 05:01-05:44

IPFIX integrates data that normally goes to Syslog or SNMP information directly in the IPFIX packet, eliminating thee additional services collecting data from each network device. IPFIX has provisions for fields that are variable-length, meaning no ID number restrictions. It came about because of the need for a standardized protocol for internal protocol flows. This data comes from routers, servers, and other network appliances that are mediation systems. The data is formatted and sent to an exporter and then on to a collector.

IPFIX, like NetFlow, looks at flow and the number of packets being sent and received during a given session.

### Summary 05:44-06:09

That's it for this lesson. In this lesson, we talked about the different ways metadata is produced. We covered some of the pitfalls of metadata as well as some of the ways to overcome them. And we discussed creating a baseline using network bandwidth monitors. These monitors can alert us to abnormal activity on our network.

## 12.3.5 Saving Captured Files with Wireshark

### Save Captured Files with Wireshark 00:00-00:46

In this demo, we'll show you how to capture network packets. There are times when you might want to capture packets so you can analyze them later. Captured packets can be used by an analyst to profile an application's network traffic or to examine a protocol in more detail.

The two most popular tools to capture packets are Wireshark and TCPDump. Both Wireshark and TCPDump can be used with a variety of operating systems but for this demo we will use Security Onion.

Security Onion is usually set up with a monitor port that captures all packets that it sees. The packets are typically used with tools like Zeek and Snort. This interface can also be used by an analyst for ad-hoc captures.

### Capture Packets with Wireshark 00:46-02:37

There are two capture tools in Security Onion for ad-hoc captures. First, we will look at Wireshark. Wireshark is a graphical tool that allows packet capture, but is also an analysis tool. Because Wireshark requires root permissions to capture the packets, we must run Wireshark with elevated permissions.

To do this, go to Applications > Utilities > Terminal. At the command prompt type —˜sudo dpkg-reconfigure wireshark-common'. You are asked you if you want to allow non-superusers to be able to capture packets. Choose Yes. Next, run —˜sudo usermod -a -G wireshark administrator'. This gives the administrative user rights to run Wireshark by adding it to the Wireshark group.

If your account on Security Onion has a different name, use that instead of administrator in the command. Once done, go to the power icon. Then, go down to the other power icon. Choose to restart.

Now the system has rebooted. After logging back into Security Onion, we can start to use Wireshark. We open it by going to `Applications > Internet > Wireshark`.

To set up a capture, select the interface on Security Onion that is set as the monitor port. In this instance, it is interface enp0s8. After selecting the interface, you can restart the capture by clicking the Shark Fin in the top left of the menu bar. By default, the captured packets will scroll by on the screen.

When we are done capturing, we can press the red stop button. At this point, we can analyze the captured packets as we would any other capture that has been given to us. We can then save that file as a PCAP file for later analysis. We're not going to save it here, but you can see we have the option.

### Capture Packets with tcpdump 02:37-03:41

Another way that we can capture packets with Security Onion is to use the command-line tool tcpdump. Let's open a new Terminal session and enter the command —˜sudo tcpdump -D' to list the possible interfaces that we can capture on. Note that enp0s8 is number 10 on the list.

To capture, run the command —`sudo tcpdump -i 10 -w testout.pcap`. This will capture the packets on enp0s8 and write them to a file called testout.pcap. Press CTRL-C to stop the capture.

Now let's print the file to the screen using —`tcpdump -r testout.pcap`. Here you can see the output of the file.

We can open this PCAP file in a tool like Wireshark or NetworkMiner. Let's open it in Wireshark. We type, —˜wireshark testout.pcap' and press Enter. After a second or two the PCAP file is loaded into Wireshark and we can use it to further analyze the packet capture.

### Summary 03:41-03:53

That's it for this demo. In this demo we used Wireshark and TCPdump to capture packets and saved them to a PCAP file.

## 12.3.6 Use Elasticsearch Logstash Kibana

### Elasticsearch Logstash Kibana (ELK) 00:00-00:44

In this demo, we use the Elasticsearch Logstash Kibana (ELK) stack to store and search security logs created by other tools in Security Onion such as Zeek. We'll use Kibana to review sample logs.

Security Onion is a free and open source Linux distribution for threat hunting, enterprise security monitoring, and log management. It includes Elasticsearch, Logstash, Kibana, Snort, Suricata, Zeek (formerly known as Bro), Wazuh, Sguil, Squert, CyberChef, NetworkMiner, and many other security tools.

### ELK Startup 00:44-01:21

First, we click the Kibana link on the Security Onion desktop. By default, Security Onion uses a self-signed TLS certificate. We tell Chromium to allow us to proceed by clicking Advanced and then proceed. On a production system, it would be advisable to install a valid TLS certificate and use the fully qualified name instead of localhost.

The username and password for Security Onion are set during the initial setup. In this case, we set up a user in the Security Operations Center (SOC). We'll login as that user.

Network Intrusion Detection System (NIDS) logs 01:21-03:15
After logging in, Kibana defaults to the dashboard page. This could be used as a dashboard for a SOC or as the starting point for a threat hunter. In this case, we want to look at the Network Intrusion Detection System (NIDS) logs, so we click NIDS to drill down.

Here we notice a classification of Attempted Administrator Privilege Gain that we want to look at further. To do that, we hover on that line and click the magnifying glass with the plus sign. This adds it as a filter. To keep the filter as we pivot to other parts of Kibana, we click Actions and pin the filter.

Next, we click Discover to see the logs that match the filter. Notice how the filter has stayed with us because we pinned it. Now we can see each log entry that matched our filter. Notice that the timestamp and other information about the alert is listed.

You can view more information for an event by clicking the arrow to the left of the time stamp. From the description, we can also search for more information about the signature listed. We do this by highlighting it, right-clicking, and choosing to search for it using Google.

Some signatures are based on Common Vulnerabilities and Exposures (CVEs). CVEs describe potential security issues with certain software or hardware. They provide a common language to evaluate the risk posed. By typing CVE into the search field, you can focus the results on signatures that are tied to a CVE. Researching the CVE can help you determine whether the attempt against the asset could be successful.

### Zeek 03:15-04:30

Let's look at one more example. But first, let's clear our pinned criteria by going to Actions and clicking Remove. We also need to remove the search term CVE. Once this is done, we'll go back to the dashboard.

Under Zeek Hunting, click HTTP to list alerts relating only to the HTTP protocol. Now we can look at the events based on the HTTP status messages and the methods used. Using the same technique with the magnifying glass as before, let's drill into the Forbidden messages.

Again, we want to pin the filter so that it stays with us as we move through Kibana. Notice that this time we are using a different way to pin the filter. This method allows you to pin or unpin individual filters instead of all filters.

In the details of one of the events, we can see the Uniform Resource Identifier (URI) that the attacker tried to access. We can also see the user agent of the attacker. In this case, our attacker didn't mask the fact that a common tool for finding website vulnerabilities called Nikto was being used.

### Summary 04:30-04:43

That's it for this demo. In this demo we went over a few of the ways the power of Elasticsearch can be harnessed to help the security professional.

## 12.3.7 Use NetworkMiner

### Use NetworkMiner 00:00-00:16

NetworkMiner is a Network Forensic Analysis Tool. It is able to take a PCAP file and analyze it for clues about the hosts and protocols on the network at the time of the capture.

### Load PCAP File 00:16-00:39

To open NetworkMiner in Security Onion, go to Applications, Other and then NetworkMiner. Once it opens, go to file open and then choose the pcap file that you want to analyze. NetworkMiner will then automatically analyze the file and present the information in a series of tabs.

### Viewing Hosts 00:39-01:48

The first tab is the hosts tab. This tab will list all of the unique hosts by IP addresses that were found in the PCAP file. Each host can be opened by clicking on the plus sign next to it revealing the information about the host that NetworkMiner knows. For example, if I open 172.28.24.3 I will get it's IP, MAC Address, information about the number of packets it sent and received, and any details about the host. In this case, if I click on the Host Details, I will see the User-Agent string of the browser the host used.

In the next tab, you will find the files that were transferred between hosts during the packet capture. Right-clicking on a file will allow you to open the file or calculate the file's hash. For this capture, I created a fake virus using the EICAR test string. This string when placed in a file will be detected as a virus by most vendors. I can choose to open the folder where the virus.exe is stored and then open it using gedit to show the EICAR string.

### Compare Hashes 01:48-02:37

I can also use the file hash feature to compare the file to known bad file hashes using tools like VirusTotal. First right-click on the file and choose Calculate MD5/SHA1/SHA256 hash. Next highlight the hash you want to compare and press CTRL+C to copy it to the clipboard. In a web browser, open VirusTotal.com and click on the Search link. Then paste the hash into the search box and press enter. This will then compare the hash against known malicious files. In this case, it will be an EICAR test file that most antivirus manufacturers will mark as a virus.

### Additional Features 02:37-03:26

Another tab of interest is the credentials tab. Here any clear text credentials like telnet, FTP, or HTTP will be shown. In this capture, there was an FTP session that we have the credentials for.

The sessions tab shows the unique conversations between hosts.

Any DNS queries and responses that are captured will be decoded in the DNS tab.

The parameters tab shows many potential key-value pairs that may have important information.

Finally, NetworkMiner provides the ability to search a PCAP file for certain keywords. You can do this by clicking on Keywords. Next add a keyword and follow the instruction to reload the case file. The display will now show all of the packets that include the keyword.

### Summary 03:26-03:38

That's it for this demo. In this demo we used NetworMiner to view the contents of a captured PCAP file.

## 12.3.8 Configuring Remote Logging on Linux

### Configuring Remote Logging 00:00-01:30

There are many aspects of the syslog daemon that you can modify to customize how your log files are configured. You could have warning messages going into one file, error messages going into another, or separate them based on program. But one of the cool things I think the syslog daemon can do is log to a remote host. This basically allows us to write our logs not only to our local system but also to a log host somewhere else. This is a great benefit for the administrator. You can set up a log host in your network somewhere and all the logs from all the systems you support go into that log host. If somebody's having a problem, instead of having to use SSH in their system to look at their log files, you can have one central location to view them. This can be very helpful in keeping history and tracking your log files. Syslog servers are also very helpful when there has been an attack on your network.

That's what we're going to do here. We're going to configure the syslog daemon to log to a remote host. I have two different Linux systems running. I have a RedHat system running here that will serve as my log host and I have a CentOS system running here that will serve as my log client. The log messages from the CentOS system are going to be saved on our syslog server which happens to be our Redhat system. Keep in mind that different Linux distributions may be slightly different. But the same concepts apply.

### Configure Log Host 01:30-03:52

Let's configure the log host first. The first thing we need to do is check to see if the rsyslog daemon is running. Type ‘systemctl status rsyslog'. As you can see everything appear to be in order. We need to change to our root user account, so I'll do a 'su- root' command and enter the password. We're going to edit the configuration file with the ‘vi /etc/rsyslog.conf' command. As we scroll down, we're going to uncomment some modules. These are the imudp module and the imtcp module. These modules enable the syslog daemon to listen for incoming syslog messages.

Down at the bottom in the rules area we have a template we're going to use for our remote logging. This template isn't here by default so it has to be added. This allows the remote logs to be placed in their own folder by host name and program name. If you don't set up a template, all syslog messages from remote hosts will go in the /var/log/messages of the syslog server. We're going to uncomment this by removing the pound symbols. Let's save it by typing ‘:wq!'.

For these changes to take effect, we must restart the rsyslog daemon by typing ‘systemctl restart rsyslog'. Since there will be incoming traffic, we need to modify the firewall to accept incoming messages on port 514.

To do so, type `firewall-cmd --permanent --add-port=514/udp`. When you press Enter you see that it was a success. Now we do the same thing with the TCP protocol with "arrow up" and remove UDP for TCP. The changes won't be active until we reload the firewall with the ‘firewall-cmd --reload' command.

I'm going to do a netstat to make sure the syslog daemon is listening on port 514.

Type `netstat -tulnp | grep "rsyslo"` and press Enter. This shows us that rsyslogd has port 514 open for UPD and TCP.

### Configure Log Client 03:52-05:06

We're going to venture over to our CentOS client server that's going to be sending syslog messages to our RedHat server. First, we check to make sure the rsyslog daemon is running by typing "systemctl status rsyslog". We're good to go.

Just like our syslog server, we're going to modify the configuration file for rsyslog. Let's type ‘sudo vi /etc/rsyslog.conf'. Enter the password and press Enter. We're going to scroll all the way to the bottom to this configuration file.

Just so that I know what this is, I'm going to put a comment in there which is "#syslog server" and type ‘_._ @@192.168.0.55:514'. The _._ is a wildcard that sends all syslog messages to the syslog server. The IP address listed is the IP of the syslog server and 514 is the port specified to use. We save this with ‘:wq!' and Enter.

`sudo systemctl restart rsyslog` allows the rsyslog daemon to grab the new settings we just edited.

### Test Syslog Messages 05:06-06:01

Now this is the fun part. We will actually get to see a message go over to our syslog server from our CentOS client server. To write a test message, we type ‘logger this is a test message' and press Enter. Let's use ‘sudo tail -f /var/log/messages' to see the message locally. Now that we see the local message, let's go over to our syslog server with ‘cd /var/log/centos-server1/'. I'm going to do a quick ‘ll' to list what log files have already been written. Since we did a logger command under the TestOut user, we're going to tail that log file to look for our newly created test message with ‘tail -f testout.log'. As you can see, it's the exact same message that was on our CentOS client server.

### Summary 06:01-06:31

That's it for this demo. In this demonstration, we talked about how to configure a log host with Linux on a network. We first looked at the steps you need to complete in order to configure the system that's going to function as the log host to receive logging messages from another system. Then we looked at the log client and configured it to send its log messages not only to its own log files, but also to send a copy to our log host.

## 12.3.9 Logging Events on pfSense

### Logging Events on pfSense 00:00-00:31

In this demo, we're going to spend a few minutes viewing log files on our pfSense security appliance. A log should act as a red flag that something is happening—potentially something bad. By reviewing your logs on a regular basis, you'll get an idea of the normal traffic on your device. In reality, no one likes to spend hours a day viewing log files. Typically, you would want to configure a syslog server so that all your logs from all devices go to one place to be consolidated for easier analysis.

### System Logs 00:31-01:24

To view logs on pfSense, we first need to go to Status and then down to System logs. Once we're in System logs, we see the General tab. Like I said, viewing logs isn't the most exciting thing to do, but it's necessary. Under the General tab we can see that we have a time stamp, a process, a PID or process ID, and a message about the log. Let's move on to Gateways.

Under Gateways, you can see that I only have one Gateway on this test system. Looks like my system is grabbing an IP for the WAN network interface. You might be thinking that if it's grabbing an IP from a WAN, shouldn't this be a public IP? That answer is yes, but I do have a test network set up and that network is connected to my regular LAN.

We also have our Routing logs here. Next to that I have my DNS Resolver logs. Finally, we have our Wireless logs. I don't have any Wi-Fi currently configured with this device, so there are no logs for our Wi-Fi.

### Firewall Logs 01:24-03:06

I'll move over to my Firewall logs. This is generally where you might look for malicious attacks directed toward your network. You'll see information down here with more details. As I scroll down, you can see the different source IPs that triggered the log. This one here, 172.16.1.100, is from my DMZ trying to get out to the WAN.

Dynamic View shows us a bit less detail. Down here I have some WAN traffic on port 5353. As you first get a system set up, you might want to familiarize yourself with the different ports that your firewall is logging. Some ports might be perfectly normal while others might not be. I wasn't familiar with port 5353, but a quick web search told me it's for multicast DNS and is safe. So now I know what it is.

Summary View gives us a bunch of graphs that can be helpful to get a quick visual of things. Here I have my different interfaces. I have three in this device—one for my LAN, one for the DMZ, and one connected for the WAN.

I have my protocols and it looks like most of my traffic is UDP.

Down here a little further, I can see the source IPs of what's been triggering the logs. All the 192.168 addresses are from the WAN interface and the 172.16 address is my DMZ.

I have the destination IPs in this next graph. I have my source ports next. Finally, I have destination ports. Here you can see UDP/53 listed. That is my DNS traffic.

So, it's good to get familiar with what your typical traffic looks like. This is called creating a baseline. This isn't covered in this demo, but it should be one of the first things you do when setting up new devices.

### DHCP Logs 03:06-03:31

My next set of logs are DHCP logs. Here you can see who's getting IPs from the DHCP server. Not only can you see who's getting IPs, but you can see the DHCP acknowledgement, the DHCP requests, and down here you can see that DHCP renewed an IP for one of the clients. All of this is useful information if you're troubleshooting DHCP or need to see what devices are connecting to your network.

### Captive Portal Authorization 03:31-03:52

I had this device set up as a captive portal at one time so that we could see all the events that are related to it. As a quick review, a captive portal is a web page you're taken to, such as in a hotel or other public place, before you're given access to the internet. You typically must agree to the terms and conditions before being allowed to proceed. You might have to enter a password as well.

### Other Logs 03:52-04:12

The next four tabs—IPSec, PPP, VPN, and Load Balancer—have no log files because I don't have any of those services running on this device. But I do have OpenVPN configured. Down here you can see all of those log files. I do have some NTP logs, or Network Time Protocol logs, here.

### Log Settings 04:12-05:40

The last thing I want to look at is log settings. If you noticed as we were viewing logs, there were only fifty shown. Here is where we can change that if needs be. As I scroll down, you can see other settings that you can configure. You can even reset your logs.

Here at the bottom is where you can configure pfSense to send these logs to a syslog server. I'll check this box to enable it. When I do, I'm presented with some more settings specific to remote logging.

I can be specific about which interface I want to log. I can log all of them, or just one. I could just log my WAN events if that's all I'm concerned about.

I can choose to only have IPv4 logs or only have IPv6 logs sent. Here I would put the IP and port of the remote syslog server that's configured to receive those logs. I don't have a server set up for this demo, but if I did, this is where I would tell pfSense to send the logs to. The format would be something like 10.10.10.100, and it would be port 514. Port 514 is the port that pfSense uses by default to send logs to the syslog server.

Now I would need to tell pfSense what I want to send. I don't like information overload. I only want to send the logs that I actually need. So I might only want firewall events, DNS events, DHCP events, and VPN. In my case, that's OpenVPN. I would then click on Save and, if my syslog server is configured, it'll start to receive log files from my pfSense security appliance.

### Summary 05:40-06:00

That's it for this demo. In this demo, we examined the logs on our pfSense security appliance. We viewed many of the different logs that can be collected. We then looked at settings and some things that we can configure. We ended by explaining how to send our logs to a syslog server.

## 12.3.10 Monitoring Data and Metadata Facts

This lesson covers the following topics:

- Bandwidth monitors
- Metadata
- Data analyzers

### Bandwidth Monitors

Today's bandwidth monitors provide a broader array of functions than simply monitoring the volume and speed of internet traffic. Bandwidth monitors can help you understand network usage, the protocols being used, users who consume a lot of bandwidth, and who is communicating on the network. When you use a bandwidth monitor, you will:

<ul>
   <li>
    Establish and update baselines. Baselines provide a reference for
        normal and abnormal activity.
   </li>
   <li>
    Create data points. To be useful and accurate, a baseline requires
        thousands of data points. Data points are customizable. You can set
        connectivity, file activity, and access attempts and failures.
   </li>
   <li>
    Set intervals. You can set intervals in minutes, hours, days, weeks,
        months, or a year. Longer monitor runs equal more data points.
   </li>
</ul>

### Metadata

Metadata is produced by almost all network activity. Server requests, applications, and email are some examples of where metadata can be found. In the context of bandwidth monitors, metadata is used to investigate security related concerns or incidents. The following table describes three types of metadata.

<table>
    <thead>
    <tr>
        <th class_="firstTableHeader" scope="col" class="fw-bold">Type</th>
        <th scope="col" class="fw-bold">Description</th>
    </tr>
    </thead>
    <tbody>
    <tr>
        <td>Email metadata</td>
        <td>
        Email provides metadata that is used to trace email. All emails come
        with a header that contains information about both the sender and
        recipient. Parts of the headers can be spoofed giving investigators
        false information. However, there are security devices that put
        X-headers throughout an email's header. These provide the
        originating email account and IP address not the spoofed one.
        </td>
    </tr>
    <tr>
        <td>Mobile metadata</td>
        <td>
        Tablets, laptops, smart phones, smart watches, and any other device
        that connects to the internet and can be moved around produces
        mobile metadata. These devices send emails, text-messages, and use
        apps. All of these produce metadata that can be used to identify
        people, places, times, and even deleted data. Pictures can be
        timestamped and geolocation stamped. Much of this metadata also
        reveals origination of the data and the sender.
        </td>
    </tr>
    <tr>
        <td>Web metadata</td>
        <td>
        Websites produce many types of metadata. The metadata on a user's
        machine versus the server can be very different. The data on both
        sides of the transmission can help fill in gaps and corroborate
        findings. Metadata includes IP addresses, user requests, user
        downloads, time spent on the site, and even attempts to gain
        unauthorized access. Web metadata includes cookies, browser history,
        and cached pages. Many times malicious actors will attempt to
        obfuscate their metadata. However, there are ways of finding the
        real metadata, especially for trained forensic investigators.
        </td>
    </tr>
    </tbody>
</table>

### Data Analyzers

Network admins should always look for a way to examine what is happening inside the network. There are a number of tools to help sift through the tremendous amounts of data generated by network activity. The following table describes some of these tools.

<table>
    <thead>
    <tr>
        <th class_="firstTableHeader" scope="col" class="fw-bold">Tool</th>
        <th scope="col" class="fw-bold">Description</th>
    </tr>
    </thead>
    <tbody>
    <tr>
        <td>NetFlow</td>
        <td>
        NetFlow is a feature on Cisco routers. It works at layers 2 – 4. It
        can examine each data flow that comes through the network or it can
        be set to sample sessions at certain intervals.
        </td>
    </tr>
    <tr>
        <td>sFlow</td>
        <td>
        sFlow is a packet sampling technology that works on layers 2 – 7 of
        the stack. Unlike NetFlow, sFlow only can be used in sampling mode.
        This is a stateless packet sampling that provides information on
        various layers and does it quickly and efficiently.
        </td>
    </tr>
    <tr>
        <td>IPfix</td>
        <td>
        IPfix directly integrates data that normally goes to Syslog or SNMP.
        This eliminates additional services collecting data from each
        network device. IPfix has provisions for fields that are variable
        length, meaning that there are no ID number restrictions. IPfix
        addresses the need for a standardized protocol for internal protocol
        flows. This data comes from routers, servers, and other network
        appliances that are mediation systems. The data is formatted, sent
        to an exporter, and then sent to a collector. IPfix, like NetFlow,
        looks at flow and the number of packets being sent and received during a given session.
        </td>
    </tr>
    </tbody>
</table>
