import{_ as s,c as n,b as a,a as e,d as t,o as l}from"./app-6Mc17HFZ.js";const o={};function r(d,i){return l(),n("div",null,i[0]||(i[0]=[a("<p>As you study this section, answer the following questions:</p><ul><li> Why is redundancy important to network security? </li><li> Why would an organization use geographic dispersal? </li><li> What are the levels of RAID and when would you use each level? </li><li> Why would a system administrator want to use load balancers? </li><li> What is an uninterrupted power supply used for? </li><li> What is the difference between active/active and active/passive? </li><li> What is the main advantage of RAID 0? Disadvantage? </li><li> What is the difference between RAID 0+1 and RAID 1+0? </li></ul><p>In this section, you will learn to:</p><ul><li> Implement RAID. </li><li> Configure fault-tolerant volumes. </li></ul><p>Key terms for this section include the following:</p>",5),e("table",{class:"terms"},[e("thead",null,[e("tr",null,[e("th",{class_:"firstTableHeader",scope:"col",class:"fw-bold"}," Term "),e("th",{scope:"col",class:"fw-bold"}," Definition ")])]),e("tbody",null,[e("tr",null,[e("td",null," Fault tolerance "),e("td",null," The ability to respond to an unexpected hardware or software failure without loss of data or loss of operation. ")]),e("tr",null,[e("td",null," Redundancy "),e("td",null," A method for providing fault tolerance by using duplicate or multiple components that perform the same function. ")]),e("tr",null,[e("td",null," Geographic dispersion "),e("td",null," Using multiple locations to store data to mitigate downtime due to loss of availability at a location. ")]),e("tr",null,[e("td",null," Multipath "),e("td",null," A fault-tolerance technique that gives multiple physical paths between a CPU and a mass-storage appliance. ")]),e("tr",null,[e("td",null," Load balancers "),e("td",null," A process that distributes processing among multiple nodes. ")]),e("tr",null,[e("td",null," Uninterrupted power supply (UPS) "),e("td",null," A stand-alone power supply that allows servers to be gracefully shutdown during a power outage. ")]),e("tr",null,[e("td",null," Virtual machine (VM) "),e("td",null," A computer that uses software components, but acts like a physical machine. A virtual machine resides on a host machine. ")]),e("tr",null,[e("td",null," Active/active "),e("td",null," Two load balancers working in tandem to distribute network traffic. ")]),e("tr",null,[e("td",null," Active/passive "),e("td",null," Two load balancers with one actively working and the second in listening mode to take over if the active machine fails. ")]),e("tr",null,[e("td",null," Virtual IP "),e("td",null," An IP address that can be used by multiple endpoints. It is commonly used in failover systems and for load balancing. ")]),e("tr",null,[e("td",null," Storage area network (SAN) "),e("td",null," A dedicated, high speed network of storage devices. Usually used for file shares. ")])])],-1),e("p",null,"This section helps you prepare for the following certification exam objectives:",-1),e("table",{class:"objectives"},[e("thead",null,[e("tr",null,[e("th",{class_:"firstTableHeader",scope:"col",class:"fw-bold"}," Exam "),e("th",{scope:"col",class:"fw-bold"}," Objective ")])]),e("tbody",null,[e("tr",null,[e("td",null," TestOut Security Pro "),e("td",null,[t(" 2.0 Physical and Network Security "),e("blockquote",null," 2.2 Harden Network Devices ")])]),e("tr",null,[e("td",null," CompTIA Security+ SY0-601 "),e("td",null,[t(" 2.5. Given a scenario, implement cybersecurity resilience. "),e("br"),e("ul",null,[e("li",null,[t(" Redundancy "),e("ul",null,[e("li",null," Geographic dispersal "),e("li",null,[t(" Disk "),e("ul",null,[e("li",null," Redundant array of inexpensive disks (RAID) levels "),e("li",null," Multipath ")])]),e("li",null,[t(" Network "),e("ul",null,[e("li",null," Load balancers "),e("li",null," Network interface card (NIC) teaming ")])]),e("li",null,[t(" Power "),e("ul",null,[e("li",null," Uninterruptible power supply (UPS) "),e("li",null," Generator "),e("li",null," Dual supply "),e("li",null," Managed power distribution units (PDUs) ")])]),e("li",null,[t(" Replication "),e("ul",null,[e("li",null," Storage area network (SAN) "),e("li",null," VM ")])]),e("li",null," On-premises vs. cloud ")])])]),t(" 3.3. Given a scenario, implement secure network designs "),e("ul",null,[e("li",null,[t(" Load balancing "),e("ul",null,[e("li",null," Active/active "),e("li",null," Active/passive "),e("li",null," Scheduling "),e("li",null," Virtual IP "),e("li",null," Persistence ")])])])])])])],-1),a('<h2 id="_12-7-1-redundancy" tabindex="-1"><a class="header-anchor" href="#_12-7-1-redundancy"><span>12.7.1 Redundancy</span></a></h2><h3 id="redundancy-00-00-00-13" tabindex="-1"><a class="header-anchor" href="#redundancy-00-00-00-13"><span>Redundancy 00:00-00:13</span></a></h3><p>Let&#39;s talk about server and site redundancy.</p><p>Redundancy is also known as fault tolerance. Its purpose is to avoid a single point of failure.</p><h3 id="server-redundancy-00-13-01-09" tabindex="-1"><a class="header-anchor" href="#server-redundancy-00-13-01-09"><span>Server Redundancy 00:13-01:09</span></a></h3><p>There are strategies to make a server redundant, or fault tolerant.</p><p>You could have redundant connections, such as two NICs; redundant parts, such as two power supplies; spare parts that are identical, waiting in case you need them; you could use an uninterruptible power supply, a UPS, and system.</p><p>Backups provide redundancy for data. For that reason, you use both on-site and off-site backups. Implement RAID levels for your hard drives. RAID 1 and RAID 5 both provide fault tolerance or redundancy.</p><p>Have backup servers known as redundant in an active-passive system. In an active-passive system, redundant servers are waiting to take over in case the active server fails. For site fault tolerance redundancy, use a backup generator for the site. Also, use redundant ISP connections that will allow a network to remain operational even after a WAN Link failure.</p><h3 id="site-redundancy-01-09-01-16" tabindex="-1"><a class="header-anchor" href="#site-redundancy-01-09-01-16"><span>Site Redundancy 01:09-01:16</span></a></h3><p>Implement alternate sites. There are three categories: hot sites, warm sites, and cold sites.</p><p>Let&#39;s take a look.</p><h3 id="hot-site-01-16-01-57" tabindex="-1"><a class="header-anchor" href="#hot-site-01-16-01-57"><span>Hot Site 01:16-01:57</span></a></h3><p>The first one is a hot site. It&#39;s also known as the Active Backup Model.</p><p>This alternate location facilitates a full recovery within minutes. If your main physical location goes down, switch over to the hot site within minutes. It&#39;s the most effective backup site for disaster recovery.</p><p>Mirror, or duplicate, all your servers, devices, cabling, and infrastructure. Hot sites often have a live active connection to your main site with data being copied across, or mirrored, to the duplicate servers. If the active site fails, the hot site is ready to go. It has all the current data on the servers. A hot site, with all the equipment and infrastructure already there, is the most expensive.</p><h3 id="warm-site-01-57-02-38" tabindex="-1"><a class="header-anchor" href="#warm-site-01-57-02-38"><span>Warm Site 01:57-02:38</span></a></h3><p>A warm site is partially configured. You have some servers, some device, some cabling, some infrastructure, but not everything. You may restore some data from backup. Some installation and recovery from backup tapes will be required.</p><p>A warm site can be a reciprocal site with another organization to keep costs low. In a reciprocal arrangement, two companies utilize the same back-end warm site. If one company experiences system failure, that company takes over the warm site. This allows both companies to save on costs. Make sure that both companies are not in the same area. Otherwise, a natural disaster could take out both companies.</p><p>Finally, there is a cold site.</p><h3 id="cold-site-02-38-02-47" tabindex="-1"><a class="header-anchor" href="#cold-site-02-38-02-47"><span>Cold Site 02:38-02:47</span></a></h3><p>It&#39;s an empty facility with little or nothing configured. It&#39;ll take the longest to recover your main site to the cold site compared to your other options.</p><h3 id="summary-02-47-03-24" tabindex="-1"><a class="header-anchor" href="#summary-02-47-03-24"><span>Summary 02:47-03:24</span></a></h3><p>No matter what type of site you decide to use, use solid documentation of what you have and what you will need at the alternate site in case of disaster.</p><p>And after a disaster occurs, always return your services from the alternate site back to your primary site, starting with the least critical functions. Continue this process until all services have been moved back to the main site.</p><p>To review, redundancy avoids a single point of failure. There are many ways to provide redundancy for servers and sites. Be aware of hot sites, warm sites, and cold sites.</p><h2 id="_12-7-2-redundancy-facts" tabindex="-1"><a class="header-anchor" href="#_12-7-2-redundancy-facts"><span>12.7.2 Redundancy Facts</span></a></h2><p>This lesson covers the following topics:</p><ul><li>Implement secure network designs</li><li>Manage redundant power options</li></ul><h3 id="implement-secure-network-designs" tabindex="-1"><a class="header-anchor" href="#implement-secure-network-designs"><span>Implement Secure Network Designs</span></a></h3><p>The following table describes secure network designs.</p>',31),e("table",null,[e("thead",null,[e("tr",null,[e("th",{class_:"firstTableHeader",scope:"col",class:"fw-bold"}," Secure Network Designs "),e("th",{scope:"col",class:"fw-bold"}," Description ")])]),e("tbody",null,[e("tr",null,[e("td",null," Load balancing "),e("td",null," A process that distributes processing among multiple nodes. ")]),e("tr",null,[e("td",null," Active/active "),e("td",null," Two load balancers working in tandem to distribute network traffic. ")]),e("tr",null,[e("td",null," Active/passive "),e("td",null," Two load balancers with one actively working and the second in listening mode to take over if the first one becomes unavailable. ")]),e("tr",null,[e("td",null," Power scheduling "),e("td",null," Power scheduling is used to configure an active redundancy. This sends power to networks when a power facility goes down. Power scheduling prevent total loss of power during catastrophic events. ")]),e("tr",null,[e("td",null," Virtual IP (VIP) "),e("td",null," An IP address that is not assigned to an endpoint. VIP is used for load balancing. It typically uses NAT IP address assignment. ")]),e("tr",null,[e("td",null," Geographic dispersal "),e("td",null," The use of multiple locations to store data to mitigate downtime due to location. ")]),e("tr",null,[e("td",null," Multipath "),e("td",null," A fault-tolerance technique that gives multiple physical paths between a CPU and a mass-storage appliance. ")])])],-1),a('<h3 id="manage-redundant-power-options" tabindex="-1"><a class="header-anchor" href="#manage-redundant-power-options"><span>Manage Redundant Power Options</span></a></h3><p>Redundant power options are vital. A network without power is useless. Common power options found in datacenters include:</p><ul><li> Uninterrupted power supply (UPS). A UPS is a stand-alone bank of batteries that allows for the graceful shutdown of network appliances when power goes out. </li><li> Generator. A generator is a large scale device that provides power for an extended period of time. Normally between 24 and 48 hours. </li><li> Dual supply. A dual power supply is common in network appliances like servers and firewalls. It allows for one failure and hot-swapping. </li><li> Managed power distribution unit (PDU). A managed power distribution unit is a rack-mounted unit that distributes power on a large scale such as a data center. </li></ul><h3 id="_12-7-3-raid" tabindex="-1"><a class="header-anchor" href="#_12-7-3-raid"><span>12.7.3 RAID</span></a></h3><h3 id="raid-00-00-00-28" tabindex="-1"><a class="header-anchor" href="#raid-00-00-00-28"><span>RAID 00:00-00:28</span></a></h3><p>RAID is an acronym for Redundant Array of Independent Disks. There are different RAID levels. Most of them provide fault tolerance for your hard disks. You can implement RAID using hardware or software. Hardware RAID is more expensive, but provides much better performance than software RAID.</p><p>We&#39;re going to take a look at some basic RAID levels: RAID 0, RAID 1, RAID 5, and some RAID combinations.</p><h3 id="raid-0-00-28-01-35" tabindex="-1"><a class="header-anchor" href="#raid-0-00-28-01-35"><span>RAID 0 00:28-01:35</span></a></h3><p>RAID 0 is disk striping, but RAID 0 by itself is not fault tolerant. There are from 2 to 32 disks in the same RAID 0 set. In this example, there are three disks, represented by cylinders. The three of disks in the RAID 0 set, becomes one drive letter. Let&#39;s call it the S drive for striping.</p><p>If you save a data file to the S drive, RAID 0 divides the data and writes it to the three drives simultaneously. One-third of your data will be on each drive. The data is divided into 64K pieces, 64 kilobytes. If your data is 640K, there will be 10 pieces of 64K each. Pieces one, two, and three are written simultaneously to each of the three disks. Then RAID 0 writes pieces four, five, and six, then pieces seven, eight, and nine, and then piece number 10.</p><p>Be aware, if one of the RAID 0 disks fails, you don&#39;t lose just one-third of your data. You lose lost all of your data. You can&#39;t reconstruct the original data unless you have all 10 pieces. The advantage of RAID 0 is performance. It provides very fast read and write capability.</p><h3 id="raid-5-01-35-02-53" tabindex="-1"><a class="header-anchor" href="#raid-5-01-35-02-53"><span>RAID 5 01:35-02:53</span></a></h3><p>Next is RAID 5. RAID 5 is also disk striping, but it&#39;s striping with parity. Parity does provide fault tolerance. RAID 5 requires 3 to 32 disks in the RAID 5 set. Because you&#39;re still doing disk striping, you have fast read-write, but less than RAID 0. The difference in speed is due to the parity information to written to the disks.</p><p>So, let&#39;s go back to our example and write the 640K information using RAID 5. You&#39;ll have 10 pieces of 64K for the data, but you lose the total amount of one disk to parity information. Every third piece written on a disk will be a parity piece. As with RAID 0, you have pieces one, two, three written simultaneously; four, five, six; seven, eight, nine; and 10.</p><p>However, in each row of blocks, every third block is a parity block. With RAID 0, there was 10 pieces total. Now, you have 16 pieces because of parity information. The advantage of having the parity information is that if a disk fails, although you&#39;ve lost one-third of your data, the parity information on the other drives can help you recreate the missing data.</p><p>When a drive in a RAID 5 set fails, your read-write access will drop off substantially, but you&#39;ll still be able to access your files. You should replace the bad disk as soon as possible and then regenerate the missing data.</p><h3 id="raid-1-02-53-03-36" tabindex="-1"><a class="header-anchor" href="#raid-1-02-53-03-36"><span>RAID 1 02:53-03:36</span></a></h3><p>RAID 1 is known as disk mirroring or sometimes as disk duplexing if your disks are on different hard drive controllers. With RAID 1, you have two disks and you mirror the data 100% on both disks. When data is written to one disk, it&#39;ll be written to the second disk.</p><p>The mirror set here will be the M drive. With RAID 1, if one disk in the mirror fails, there is a full copy of the data on the other disk. It&#39;s a fault tolerant solution. However, you lose 50% of your data capacity for that fault tolerance.</p><p>With RAID 5, you lose one disk total to parity information, which is less than 50%. Because you lose 50 % of your data capacity to fault tolerance, that makes it a more expensive solution.</p><h3 id="raid-0-1-03-36-04-06" tabindex="-1"><a class="header-anchor" href="#raid-0-1-03-36-04-06"><span>RAID 0+1 03:36-04:06</span></a></h3><p>Now, let&#39;s take a look at combinations. There&#39;s RAID 0+1. In this solution, you mirror stripe sets.</p><p>For example, you have two stripe sets, one is D drive and the other is E drive. As data is stored to the D drive, it gets striped to these two disks in the stripe set. If data is stored to the E drive, it gets striped to these two drives. We&#39;ve got two separate RAID 0 sets.</p><p>Now, you mirror the two stripe sets. So, the data on the D drive will be identical to the data on the E drive.</p><h3 id="raid-1-0-04-06-04-23" tabindex="-1"><a class="header-anchor" href="#raid-1-0-04-06-04-23"><span>RAID 1+0 04:06-04:23</span></a></h3><p>Another common RAID combination is RAID 1+0. In this solution, you start with mirrors and then stripe them. With RAID 1+0, you start with mirrors that are independent and stripe them with RAID 0.</p><p>Another way to think about this is to read the combinations backwards, we are striping a mirror set.</p><h3 id="summary-04-23-04-39" tabindex="-1"><a class="header-anchor" href="#summary-04-23-04-39"><span>Summary 04:23-04:39</span></a></h3><p>So, these are some of the common RAID levels you&#39;ll encounter: RAID 0, RAID 1, RAID 5, and some combinations, RAID 0+1 and RAID 1+0.</p><h2 id="_12-7-4-implementing-raid" tabindex="-1"><a class="header-anchor" href="#_12-7-4-implementing-raid"><span>12.7.4 Implementing RAID</span></a></h2><h3 id="implementing-raid-00-00-00-26" tabindex="-1"><a class="header-anchor" href="#implementing-raid-00-00-00-26"><span>Implementing RAID 00:00-00:26</span></a></h3><p>In this demonstration, we&#39;re going to talk about managing disks and volumes. In Windows Server, right-click the Windows Start button and go to Disk Management. Let&#39;s expand this a little bit. We can see there are hard drives that have been added to this computer, but they&#39;re currently offline. Not all disks come in offline, but we can bring them back online by right-clicking and selecting Online.</p><h3 id="partition-map-00-26-01-01" tabindex="-1"><a class="header-anchor" href="#partition-map-00-26-01-01"><span>Partition Map 00:26-01:01</span></a></h3><p>The partition map for these drives is Master Boot Record, or MBR. One thing to keep in mind is that MBR disks are limited to two terabytes. You can use the GUID partition table (GPT disks) if your disks are two terabytes or larger. If you don&#39;t have a two-terabyte volume, I&#39;d recommend MBR.</p><p>Many data recovery programs don&#39;t function in the newer versions of the operating system, so they may not be able to help you with data recovery. If you don&#39;t need GPT, use MBR. You can convert the disk later.</p><h3 id="convert-a-disk-01-01-01-26" tabindex="-1"><a class="header-anchor" href="#convert-a-disk-01-01-01-26"><span>Convert a Disk 01:01-01:26</span></a></h3><p>To convert a disk, right-click it and go to Convert to GPT. It&#39;s that simple. As long as there are no volumes, you can convert it between MBR and GPT whenever you want. After you create a volume, you can&#39;t switch. You have to wipe the disk.</p><p>These disks are all basic disks. If you&#39;re using them as individual hard drives, then leave them basic. You can create different types of volumes on a basic disk.</p><h3 id="simple-volume-01-26-01-55" tabindex="-1"><a class="header-anchor" href="#simple-volume-01-26-01-55"><span>Simple Volume 01:26-01:55</span></a></h3><p>A simple volume is just a portion of disk space. If we right-click on the allocated space and then click <code>Simple Volume &gt; Next</code>, we&#39;ll make a simple volume of just 300 megabytes. Now we have to assign the drive a letter. Let&#39;s choose drive F. We&#39;re going to leave it in NTFS format. We have a simple volume labeled drive F that&#39;s only 300 megabytes, and you can see it&#39;s NTFS.</p><h3 id="extend-a-volume-01-55-02-27" tabindex="-1"><a class="header-anchor" href="#extend-a-volume-01-55-02-27"><span>Extend a Volume 01:55-02:27</span></a></h3><p>If we want to, we can right-click the disk and click Extend or Shrink the Volume, but the volume must be to an NTFS partition. There also has to be enough free space on the drive.</p><p>If you&#39;re going to extend the unallocated space next to the volume that we can see, we have to check and make sure there&#39;s plenty of space on that unallocated volume. Right now, there is.</p><p>If there were another volume in between drive F and the unallocated space right here, Microsoft would recommend you back up that volume, delete it, expand drive F, and then bring the other volume in from your backup.</p><h3 id="spanned-volume-02-27-04-02" tabindex="-1"><a class="header-anchor" href="#spanned-volume-02-27-04-02"><span>Spanned Volume 02:27-04:02</span></a></h3><p>A spanned volume uses space from two different disks. It fills one disk and then spills into the next disk. Right-click and select Spanned volume on the unallocated space. Click Next.</p><p>We can add Disk 2 and specify that it use 200 megabytes for Disk 1 and 200 from Disk 2. We need to assign it another drive letter, so let&#39;s assign this letter G. It gives us an error that says, &quot;The operation selected will convert the selected disk to dynamic disk. If you convert the disk to dynamic, you will not be able to install the operating systems from any volume on the disk.&quot; We&#39;re going to click OK because we&#39;re not going to do any of that.</p><p>When we expand a volume, it&#39;s going to select the first drive (and the first partition that we created) and fill it. Once it fills, it&#39;ll spill over into the second.</p><p>Any time we have two or more disks working together like that, we can&#39;t use basic disks. We need dynamic disks, so we have to accept that when creating these two. The downside to expanding a disk is that if one portion, or one of these drives, fails, this whole G volume fails. Expanding doesn&#39;t really provide any type of fault tolerance at all. You can also manually convert a disk to dynamic by right-clicking the disk. Let&#39;s right-click Disk 3 and select Convert disk to dynamic.</p><h3 id="striped-volume-04-02-04-55" tabindex="-1"><a class="header-anchor" href="#striped-volume-04-02-04-55"><span>Striped Volume 04:02-04:55</span></a></h3><p>Let&#39;s talk about a striped volume. A striped volume spreads data across the number of disks you specify. Let&#39;s right-click Unallocated Space on Disk 1, and then click <code>New Striped Volume &gt; Next</code>.</p><p>You&#39;ll need a minimum of two disks. There will be an equal amount of space on the disks because the computer will save equal amounts of data to each of them. Let&#39;s add Disk 2, another 300, and click Next. Let&#39;s do drive H. With those two disks, half goes on Disk 1, and half goes on Disk 2. This is a performance benefit, but it doesn&#39;t provide any fault tolerance.</p><p>If either of these disks fails, then the striped volume fails. If you want fault tolerance for a Windows system, you have to create a mirrored volume or a RAID 5 volume.</p><h3 id="raid-5-volume-04-55-06-29" tabindex="-1"><a class="header-anchor" href="#raid-5-volume-04-55-06-29"><span>RAID 5 Volume 04:55-06:29</span></a></h3><p>Let&#39;s go ahead and go to New mirrored volume and click Next. We&#39;ll choose 300 megabytes from each disk. For mirrored, we can only do two disks. Now select drive E. While we&#39;re choosing 300 megabytes from each disk, the volume is only 300 megabytes total because 100 percent of the data will be saved to both disks. If one disk fails, then we have a perfect mirror on the other disk.</p><p>We&#39;re also going to create the RAID 5 volume. Let&#39;s right-click on this unallocated space and go to New RAID 5 Volume. A RAID 5 volume requires at least three disks. RAID 5 is also known as striping with parity. If we choose 300 megabytes for each disk, one of these disks will be used for the parity information. While you lose one disk, the parity is spread across all disks. With three disks, you&#39;ll lose 33 percent of the space. With four disks, you&#39;ll lose 25 percent. With five, you&#39;ll lose 20 percent. So, the more disks in a RAID 5 array, the more efficiently you use space. Using RAID 5, you can lose up to one disk, and the drive will still keep functioning. It&#39;s going to be slower, but it&#39;ll work.</p><p>Let&#39;s add the other two drives. Let&#39;s go ahead and just add the same 300. And then click Next. We&#39;ll change this to J.</p><h3 id="summary-06-29-06-54" tabindex="-1"><a class="header-anchor" href="#summary-06-29-06-54"><span>Summary 06:29-06:54</span></a></h3><p>And that&#39;s the end of this demo. In this demonstration, we used disk management to create a simple volume and show you how to extend or shrink a volume. We discussed how to create a new spanned volume. We went over how to convert disks to dynamic. We saw how to create a new striped volume and a new mirrored volume. And we finished with how to configure a RAID 5 volume.</p><h2 id="_12-7-5-raid-facts" tabindex="-1"><a class="header-anchor" href="#_12-7-5-raid-facts"><span>12.7.5 RAID Facts</span></a></h2><p>Redundant Array of Independent Disks (RAID), also called Redundant Array of Inexpensive Disks, is a disk subsystem that combines multiple physical disks into a single logical storage unit.</p><p>This lesson covers the topic of Redundant Array of Independent Disks</p><h3 id="redundant-array-of-independent-disks" tabindex="-1"><a class="header-anchor" href="#redundant-array-of-independent-disks"><span>Redundant Array of Independent Disks</span></a></h3><p>Depending on the configuration, a RAID array can improve performance, provide fault tolerance, or both. RAID can be implemented through hardware, using a special RAID disk controller, or software. Hardware RAID is more expensive, but provides much better performance than software RAID.</p>',64),e("table",null,[e("thead",null,[e("tr",null,[e("th",{class_:"firstTableHeader",scope:"col",class:"fw-bold"}," RAID Configuration "),e("th",{scope:"col",class:"fw-bold"}," Description ")])]),e("tbody",null,[e("tr",null,[e("td",null,[t(" RAID 0 "),e("br"),t(" striping ")]),e("td",null,[t(" A stripe set breaks data into units and stores the units across a series of disks by reading and writing to all disks simultaneously. Striping: "),e("ul",null,[e("li",null," Provides an increase in performance. "),e("li",null," Does not provide fault tolerance. "),e("li",null," Does not protect data. A failure of one disk in the set means all data is lost. "),e("li",null," Requires a minimum of two disks and may contain up to 32 disks. "),e("li",null," Has no overhead because all disk space is available for storing data. "),e("li",null," Is the fastest of all RAID types. However, it does not provide fault tolerance. ")])])]),e("tr",null,[e("td",null,[t(" RAID 5 "),e("br"),t(" striping with parity ")]),e("td",null,[t(" A RAID 5 volume combines disk striping across multiple disks with parity for data redundancy. Parity information is stored on each disk. If a single disk fails, its data can be recovered using the parity information stored on the remaining disks. RAID 5: "),e("ul",null,[e("li",null," Provides an increase in performance. "),e("li",null," Provides fault tolerance. "),e("li",null," Does not provide fault tolerance if two or more disks fail. "),e("li",null," Requires a minimum of three disks. "),e("li",null," Has an overhead of one disk in the set for parity information (1 / n - 1). A set with three disks has 33% overhead. A set with four disks has 25% overhead. A set with five disks has 20% overhead ")])])]),e("tr",null,[e("td",null,[t(" RAID 1 "),e("br"),t(" mirroring ")]),e("td",null,[t(" A mirrored volume stores data to two duplicate disks simultaneously. If one disk fails, data is present on the other disk and the system switches immediately from the failed disk to the functioning disk. RAID 1: "),e("ul",null,[e("li",null," Provides fault tolerance. Does not increase performance. "),e("li",null," Requires two disks. "),e("li",null," Has a 50% overhead. "),e("li",null," Writes data twice, meaning that half of the disk space is used to store the copy of the data. "),e("li",null," Has overhead of 1 / n where n is the price of the second disk. "),e("li",null," Is the most expensive fault tolerant system. ")])])]),e("tr",null,[e("td",null," RAID 0+1 "),e("td",null,[t(" RAID 0+1 combines disk striping (0) and disk mirroring (1). Multiple disks are striped, creating a single volume. A second set of disks is then added to mirror the first set. RAID 0+1: "),e("ul",null,[e("li",null," Provides fault tolerance. "),e("li",null," Protects data if one or more disks in a single set fails. "),e("li",null," Does not protect data if two disks in different mirrored sets fail. "),e("li",null," Provides an increase in performance. "),e("li",null," Requires an even number of disks with a minimum of four disks. "),e("li",null," Has a 50% overhead. ")])])]),e("tr",null,[e("td",null,[t(" RAID 1+0 "),e("br"),t(" mirroring a striped set ")]),e("td",null,[t(" RAID 1+0 combines disk mirroring (1) and disk striping (0). Multiple disks are configured into two mirrored arrays. The mirrored set are striped across the other set. RAID 1+0: "),e("ul",null,[e("li",null," Provides fault tolerance. "),e("li",null," Protects data if one or more disks in a single set fails. "),e("li",null," Protects data if two disks in different sets fail. "),e("li",null," Provides an increase in performance. "),e("li",null," Requires an even number of disks and a minimum of four disks. "),e("li",null," Has a 50% overhead. "),e("li",null," Is the fastest, most fault tolerant, and most expensive RAID arrays. "),e("li",null," Performs better and provides more fault tolerance than RAID 0+1 arrays. ")])])])])],-1),a('<h2 id="_12-7-7-hardware-clustering" tabindex="-1"><a class="header-anchor" href="#_12-7-7-hardware-clustering"><span>12.7.7 Hardware Clustering</span></a></h2><h3 id="clustering-00-00-00-27" tabindex="-1"><a class="header-anchor" href="#clustering-00-00-00-27"><span>Clustering 00:00-00:27</span></a></h3><p>I&#39;ll spend a few minutes talking about clustering. Clustering can be an effective way to implement a disaster-recovery plan as well as a way to improve your productivity. A cluster is a group of interconnected servers, also known as nodes, that appear to be a single system to the operating environment. Although clustering can be done on virtual machines, I&#39;ll focus this lesson on the unique characteristics of clustering on physical computers.</p><h3 id="clustering-benefits-00-27-01-10" tabindex="-1"><a class="header-anchor" href="#clustering-benefits-00-27-01-10"><span>Clustering Benefits 00:27-01:10</span></a></h3><p>Hardware clustering provides several benefits. For example, when you use clustering, the throughput and response time are dramatically improved. Since the cluster&#39;s nodes appear to be one system, if one node fails, the others in the cluster still provide the services you need and redistribute the workload among the remaining servers. To provide additional performance, more nodes can be added. Theoretically, there&#39;s no limit to the number of nodes you can have in your cluster. But you must have software that supports clustering. This software could be built into the operating system itself, such as with Windows Server 2019. In other cases, you may have to purchase a program to help you set up and manage your clusters.</p><p>With that introduction, I&#39;ll show you how clustering works.</p><h3 id="clustering-implementation-01-10-02-09" tabindex="-1"><a class="header-anchor" href="#clustering-implementation-01-10-02-09"><span>Clustering Implementation 01:10-02:09</span></a></h3><p>In a typical clustering implementation, there are at least two nodes. They can be connected in multiple ways in order to act seamlessly with each other. First, since the nodes provide services to the workstations that reside on the production network, the clustered nodes are directly connected to the production network. For example, a user at this workstation can access the services on the clustered nodes through the production network.</p><p>To increase performance, clustered nodes often have a second network card, allowing them to also be connected to each other through a dedicated network. As you can see here, this network is isolated from the production network. This means that the clustered nodes connected to this dedicated network don&#39;t have to compete for bandwidth with the production traffic. When you work with high-availability clusters, this network is also referred to as a heartbeat network. I&#39;ll talk more about that in a bit. But, keep in mind that although this is the ideal setup, you don&#39;t have to use this second dedicated network. Instead, you could choose to do your clustering over the production network.</p><h3 id="common-storage-02-09-02-49" tabindex="-1"><a class="header-anchor" href="#common-storage-02-09-02-49"><span>Common Storage 02:09-02:49</span></a></h3><p>Depending on the cluster type you choose and the cluster&#39;s purpose, the nodes in your cluster could also share a common storage that&#39;s accessible through a storage area network, or SAN. When used, a SAN is often connected to the cluster using fiber-channel connections, which are fiber optic. These connections allow the devices to communicate very quickly with the shared storage. In this setup, the shared storage appears to the operating system as if it were storage installed within the server itself. But in reality, both servers share the same disk storage. This has important implications. It means that whenever Server A writes information to the SAN, it&#39;s immediately available to Server B and vice versa.</p><h3 id="high-availability-clusters-02-49-04-50" tabindex="-1"><a class="header-anchor" href="#high-availability-clusters-02-49-04-50"><span>High-Availability Clusters 02:49-04:50</span></a></h3><p>When planning your cluster, keep in mind that there are a few different types of clusters you can implement.</p><p>One commonly used cluster is called a high-availability cluster, or HA cluster. This type of cluster is also known as a failover cluster. The idea behind this specific cluster type is to eliminate downtime when a computer system in the cluster fails. Although the most common size of an HA cluster is two nodes, an HA cluster can have many more.</p><p>A high-availability cluster typically uses what&#39;s known as an active/passive configuration. With this type of configuration, the active server, or primary server, provides the services to the production network while the passive server, or standby server, waits in the background. If the primary server fails, the passive server becomes the new active server and provides the services to the production network. In this type of configuration, the passive node must be a fully redundant instance of the active node and use the same shared storage. This way, any node in the cluster has access to the same data.</p><p>To monitor when the passive server should take over, HA clusters make sure that the other servers in the cluster are alive by sending what are called heartbeats over a dedicated heartbeat network. For example, Server A, our primary server, continually lets Server B know that it&#39;s up and running by sending these heartbeats. If Server A fails, Server B no longer hears a heartbeat coming from Server A and assumes that Server A has gone down. It immediately takes over and becomes the new active server. This is possible because both servers use the same shared storage.</p><p>Depending on how close together your heartbeat intervals are set, it may take only a few seconds for a passive server to start providing the services that the active server was providing. As such, the user usually notices little to no downtime. But, keep in mind that whatever was in RAM on the failed server is lost. If the server that failed is fixed and brought back online, it becomes the passive server and listens for heartbeats from the current active server.</p><h3 id="load-balancing-clusters-04-50-06-14" tabindex="-1"><a class="header-anchor" href="#load-balancing-clusters-04-50-06-14"><span>Load-Balancing Clusters 04:50-06:14</span></a></h3><p>Another type of cluster you can use is called a load-balancing cluster, which works differently from a high-availability cluster. In a load-balancing cluster, all nodes are always active participants. This is known as an active/active configuration. In this type of cluster, all computers share in the processing workload. In a way, you can think of a load-balancing cluster as a type of supercomputer system. In other words, the processing tasks are distributed among all the nodes within the cluster. Companies that provide web server access to a large clientele typically implement load-balancing clusters to assign the many different queries to different nodes. This optimizes the responses to these requests.</p><p>Let&#39;s look at an example.</p><p>First, notice that instead of using a SAN to provide a common disk storage, each computer has its own disk storage. This isn&#39;t a requirement of a load-balancing cluster, but since a SAN can be expensive, some companies might not choose to use them. Still, using a SAN is probably the fastest and most effective way to implement a cluster when feasible.</p><p>In some cases, load-balancing clusters might also have a separate device known as a load balancer, which is used to determine which cluster node gets the current request. Load balancing uses an algorithm to determine which server in the cluster should service the request.</p><p>Similar to the example, some implementations use a round-robin approach where Server A gets the first request, Server B gets the second, and Server A gets the third.</p><h3 id="cluster-linking-06-14-06-46" tabindex="-1"><a class="header-anchor" href="#cluster-linking-06-14-06-46"><span>Cluster Linking 06:14-06:46</span></a></h3><p>The systems in a load-balancing cluster can be loosely linked or tightly linked. The tighter the link, the more they act as one computer system. In a loosely linked cluster, each system operates autonomously, but also in conjunction with the other systems at the same time. In a tightly linked system, the systems function as one system called a supercomputing cluster. These systems pool their CPU and storage resources, and they might even pool their memory together so that various processing tasks are distributed between the cluster members.</p><h3 id="hardware-compatibility-06-46-07-29" tabindex="-1"><a class="header-anchor" href="#hardware-compatibility-06-46-07-29"><span>Hardware Compatibility 06:46-07:29</span></a></h3><p>A key thing to remember when you implement clustering is that the more tightly integrated the systems, the more identical the hardware needs to be.</p><p>If you&#39;re using a loosely linked cluster, you can use hardware that&#39;s slightly more disparate. For example, you can use servers from different manufacturers. But, to implement a tightly linked cluster, the systems need to be identical. So, they should be from the same manufacturer, the same make and model, same processor, same amount of storage, same amount of RAM, and so on.</p><p>I don&#39;t have time to go into specific clustering implementations on various operating systems. Just be aware that most of your commonly used network operating systems do have some type of clustering solution available, whether it&#39;s built into the product itself or whether it&#39;s available from a third party.</p><h3 id="summary-07-29-07-53" tabindex="-1"><a class="header-anchor" href="#summary-07-29-07-53"><span>Summary 07:29-07:53</span></a></h3><p>That&#39;s it for this lesson. In this lesson, we gave you an overview of how clustering works. We talked about the role of a cluster and several clustering implementations. Then we looked at high-availability and load-balancing cluster types, and we talked about how you create a supercomputing cluster, or tightly linked cluster, from a load-balancing cluster.</p><h2 id="_12-7-8-clustering-facts" tabindex="-1"><a class="header-anchor" href="#_12-7-8-clustering-facts"><span>12.7.8 Clustering Facts</span></a></h2><p>This lesson covers the following topics:</p><ul><li> Hardware clustering concepts </li><li> High availability clustering </li><li> Load balancing clustering </li></ul><h3 id="hardware-clustering-concepts" tabindex="-1"><a class="header-anchor" href="#hardware-clustering-concepts"><span>Hardware Clustering Concepts</span></a></h3><p>Hardware clustering connects a group of independent computers to increase the availability of applications and services. Each clustered server is called a node. The nodes are connected physically by cables and use software to monitor and maintain the connections.</p><ul><li> Clusters typically use a storage area network (SAN) to provide access to the shared storage. Cluster members have a network connection to the regular network to respond to client requests. They also have a network connection to the SAN to access the shared storage. </li><li> The cluster is identified by a shared IP address. Client requests are directed to the shared IP address, not the IP address of an individual cluster member. </li><li> Cluster members communicate by sending periodic heartbeat signals to each other. They use the heartbeat signals to maintain consistent information about cluster membership. </li><li><i class="fs-italicize"> Convergence </i> is the process that cluster members use to reach a consistent state. In a consistent state, all cluster members are aware of all other members and the client load has been distributed between cluster members according to the load balancing rules. </li><li> Clustering helps to ensure a service is accessible most of the time. Clustering is a form of high availability. </li><li><i class="fs-italicize"> Elasticity </i> is the level of difficulty involved when removing nodes from the data store. </li><li><i class="fs-italicize"> Scalability </i> is a system&#39;s ability to handle a growing level of work. </li></ul><h3 id="high-availability-clustering" tabindex="-1"><a class="header-anchor" href="#high-availability-clustering"><span>High Availability Clustering</span></a></h3><p>A high availability cluster (HA), also known as a failover cluster, is a group of computers configured with the same service. In HA clusters:</p><ul><li> One node is configured as the active node; other nodes are configured as passive nodes. </li><li> The passive nodes must be a fully redundant instance of the active node. </li><li> The active node provides the requested information to network users. The passive nodes are inactive, but are available when needed. </li><li> Active and passive nodes continually communicate via heartbeats and are connected to the same shared storage. When the active node fails, a passive node takes over. </li><li> A single point of failure is eliminated through the use of redundant nodes. </li></ul><h3 id="load-balancing-clustering" tabindex="-1"><a class="header-anchor" href="#load-balancing-clustering"><span>Load Balancing Clustering</span></a></h3><p>A load balancing cluster disperses a workload between two or more computers (nodes) to achieve optimal resource utilization, throughput, or response time. Load balancing improves performance by distributing the workload between multiple servers/nodes. Load balancing also provides fault tolerance. If one server is unavailable, additional servers are available to fulfill the request. When working with load balancing clusters, consider the following:</p>',42),e("ul",null,[e("li",null," All of the nodes in a load balancing cluster are active participants at all times. "),e("li",null," All of the processing tasks to be completed are distributed between all of the nodes in the cluster. "),e("li",null," Depending on the implementation, nodes can share processing capabilities, storage, and system RAM. "),e("li",null," Nodes in a load balancing cluster can be tightly or loosely linked. The tighter the link, the more the nodes function as one system. "),e("li",null," A tightly linked load balancing cluster is known as a supercomputing cluster. "),e("div",{class:"info","data-block":`
    The more tightly linked the nodes in the cluster are, the more identical the nodes need to
        be.
   `},[e("div",{class:"to-info-box"},[e("div",{class:"to-info-box-body"},[e("div",{class:"to-icon large","aria-hidden":"true",style:{width:"20px"}},[e("svg",{"aria-hidden":"true",focusable:"false","data-prefix":"fal","data-icon":"circle-info",class:"svg-inline--fa fa-circle-info",role:"img",xmlns:"http://www.w3.org/2000/svg",viewBox:"0 0 512 512"},[e("path",{fill:"currentColor",d:"M256 32a224 224 0 1 1 0 448 224 224 0 1 1 0-448zm0 480A256 256 0 1 0 256 0a256 256 0 1 0 0 512zM208 352c-8.8 0-16 7.2-16 16s7.2 16 16 16l96 0c8.8 0 16-7.2 16-16s-7.2-16-16-16l-32 0 0-112c0-8.8-7.2-16-16-16l-40 0c-8.8 0-16 7.2-16 16s7.2 16 16 16l24 0 0 96-32 0zm48-168a24 24 0 1 0 0-48 24 24 0 1 0 0 48z"})])]),e("div",{class:"to-info-box-body-content"},[e("span",null," The more tightly linked the nodes in the cluster are, the more identical the nodes need to be. ")])])])])],-1),e("p",null,"The intent of load balancing is to virtualize a service, such as a web or a database service, offered by multiple servers. If the servers are not clustered with load balancing capabilities, a separate load balancer can be used. The load balancer forwards the service request from a client to a single member of the cluster. It chooses or schedules the member based on one of the following types of algorithms.",-1),e("table",null,[e("thead",null,[e("tr",null,[e("th",{class_:"firstTableHeader",scope:"col",class:"fw-bold"}," Algorithm "),e("th",{scope:"col",class:"fw-bold"}," Description ")])]),e("tbody",null,[e("tr",null,[e("td",null," Round robin "),e("td",null," There is no priority for selecting a member. Each member receives an equal share of requests portioned out in a circular order. ")]),e("tr",null,[e("td",null," Affinity "),e("td",null," A member is selected based on an affinity or relationship. For example, when it is desirable to send all service requests from a user to the same cluster member, an affinity can be established based on the IP address of the client or the class C address space of the client IP address. ")]),e("tr",null,[e("td",null," Least connections "),e("td",null," The member with the least number of connections is chosen. ")]),e("tr",null,[e("td",null," Least response time "),e("td",null," The member who responds most quickly to a request is chosen. ")])])],-1),e("p",null,"For higher availability, two load balancers can be used in either an active/passive mode or active/active mode.",-1),e("table",null,[e("thead",null,[e("tr",null,[e("th",{class_:"firstTableHeader",scope:"col",class:"fw-bold"}," Mode "),e("th",{scope:"col",class:"fw-bold"}," Description ")])]),e("tbody",null,[e("tr",null,[e("td",null," Active/passive "),e("td",null," One load balancer is active and handles all the service request. The passive load balancer is in listening mode and monitors the performance of the active load balancer. If the active load balancer fails, the passive load balancer become active and takes over the load balancing duties. ")]),e("tr",null,[e("td",null," Active/active "),e("td",null," Both load balancers work as a team to distribute the service requests. ")])])],-1),e("p",null,"In a high availability implementation, multiple load balancers can be clustered in the same way as other server clusters.",-1),e("p",null,"In load balancing, a Virtual IP (VIP) is an address presented to the outside world, but doesn't correspond to an actual physical network interface. To the client, the VIP responds like any normal IP address. The load balancing environment is responsible for forwarding service requests from the client to a physical server that responds to the request.",-1)]))}const h=s(o,[["render",r],["__file","12.07.Redundancy.html.vue"]]),u=JSON.parse('{"path":"/12/12.07.Redundancy.html","title":"Section 12.7 Redundancy","lang":"zh-CN","frontmatter":{"lang":"zh-CN","title":"Section 12.7 Redundancy","description":"some description"},"headers":[{"level":2,"title":"12.7.1 Redundancy","slug":"_12-7-1-redundancy","link":"#_12-7-1-redundancy","children":[{"level":3,"title":"Redundancy 00:00-00:13","slug":"redundancy-00-00-00-13","link":"#redundancy-00-00-00-13","children":[]},{"level":3,"title":"Server Redundancy 00:13-01:09","slug":"server-redundancy-00-13-01-09","link":"#server-redundancy-00-13-01-09","children":[]},{"level":3,"title":"Site Redundancy 01:09-01:16","slug":"site-redundancy-01-09-01-16","link":"#site-redundancy-01-09-01-16","children":[]},{"level":3,"title":"Hot Site 01:16-01:57","slug":"hot-site-01-16-01-57","link":"#hot-site-01-16-01-57","children":[]},{"level":3,"title":"Warm Site 01:57-02:38","slug":"warm-site-01-57-02-38","link":"#warm-site-01-57-02-38","children":[]},{"level":3,"title":"Cold Site 02:38-02:47","slug":"cold-site-02-38-02-47","link":"#cold-site-02-38-02-47","children":[]},{"level":3,"title":"Summary 02:47-03:24","slug":"summary-02-47-03-24","link":"#summary-02-47-03-24","children":[]}]},{"level":2,"title":"12.7.2 Redundancy Facts","slug":"_12-7-2-redundancy-facts","link":"#_12-7-2-redundancy-facts","children":[{"level":3,"title":"Implement Secure Network Designs","slug":"implement-secure-network-designs","link":"#implement-secure-network-designs","children":[]},{"level":3,"title":"Manage Redundant Power Options","slug":"manage-redundant-power-options","link":"#manage-redundant-power-options","children":[]},{"level":3,"title":"12.7.3 RAID","slug":"_12-7-3-raid","link":"#_12-7-3-raid","children":[]},{"level":3,"title":"RAID 00:00-00:28","slug":"raid-00-00-00-28","link":"#raid-00-00-00-28","children":[]},{"level":3,"title":"RAID 0 00:28-01:35","slug":"raid-0-00-28-01-35","link":"#raid-0-00-28-01-35","children":[]},{"level":3,"title":"RAID 5 01:35-02:53","slug":"raid-5-01-35-02-53","link":"#raid-5-01-35-02-53","children":[]},{"level":3,"title":"RAID 1 02:53-03:36","slug":"raid-1-02-53-03-36","link":"#raid-1-02-53-03-36","children":[]},{"level":3,"title":"RAID 0+1 03:36-04:06","slug":"raid-0-1-03-36-04-06","link":"#raid-0-1-03-36-04-06","children":[]},{"level":3,"title":"RAID 1+0 04:06-04:23","slug":"raid-1-0-04-06-04-23","link":"#raid-1-0-04-06-04-23","children":[]},{"level":3,"title":"Summary 04:23-04:39","slug":"summary-04-23-04-39","link":"#summary-04-23-04-39","children":[]}]},{"level":2,"title":"12.7.4 Implementing RAID","slug":"_12-7-4-implementing-raid","link":"#_12-7-4-implementing-raid","children":[{"level":3,"title":"Implementing RAID 00:00-00:26","slug":"implementing-raid-00-00-00-26","link":"#implementing-raid-00-00-00-26","children":[]},{"level":3,"title":"Partition Map 00:26-01:01","slug":"partition-map-00-26-01-01","link":"#partition-map-00-26-01-01","children":[]},{"level":3,"title":"Convert a Disk 01:01-01:26","slug":"convert-a-disk-01-01-01-26","link":"#convert-a-disk-01-01-01-26","children":[]},{"level":3,"title":"Simple Volume 01:26-01:55","slug":"simple-volume-01-26-01-55","link":"#simple-volume-01-26-01-55","children":[]},{"level":3,"title":"Extend a Volume 01:55-02:27","slug":"extend-a-volume-01-55-02-27","link":"#extend-a-volume-01-55-02-27","children":[]},{"level":3,"title":"Spanned Volume 02:27-04:02","slug":"spanned-volume-02-27-04-02","link":"#spanned-volume-02-27-04-02","children":[]},{"level":3,"title":"Striped Volume 04:02-04:55","slug":"striped-volume-04-02-04-55","link":"#striped-volume-04-02-04-55","children":[]},{"level":3,"title":"RAID 5 Volume 04:55-06:29","slug":"raid-5-volume-04-55-06-29","link":"#raid-5-volume-04-55-06-29","children":[]},{"level":3,"title":"Summary 06:29-06:54","slug":"summary-06-29-06-54","link":"#summary-06-29-06-54","children":[]}]},{"level":2,"title":"12.7.5 RAID Facts","slug":"_12-7-5-raid-facts","link":"#_12-7-5-raid-facts","children":[{"level":3,"title":"Redundant Array of Independent Disks","slug":"redundant-array-of-independent-disks","link":"#redundant-array-of-independent-disks","children":[]}]},{"level":2,"title":"12.7.7 Hardware Clustering","slug":"_12-7-7-hardware-clustering","link":"#_12-7-7-hardware-clustering","children":[{"level":3,"title":"Clustering 00:00-00:27","slug":"clustering-00-00-00-27","link":"#clustering-00-00-00-27","children":[]},{"level":3,"title":"Clustering Benefits 00:27-01:10","slug":"clustering-benefits-00-27-01-10","link":"#clustering-benefits-00-27-01-10","children":[]},{"level":3,"title":"Clustering Implementation 01:10-02:09","slug":"clustering-implementation-01-10-02-09","link":"#clustering-implementation-01-10-02-09","children":[]},{"level":3,"title":"Common Storage 02:09-02:49","slug":"common-storage-02-09-02-49","link":"#common-storage-02-09-02-49","children":[]},{"level":3,"title":"High-Availability Clusters 02:49-04:50","slug":"high-availability-clusters-02-49-04-50","link":"#high-availability-clusters-02-49-04-50","children":[]},{"level":3,"title":"Load-Balancing Clusters 04:50-06:14","slug":"load-balancing-clusters-04-50-06-14","link":"#load-balancing-clusters-04-50-06-14","children":[]},{"level":3,"title":"Cluster Linking 06:14-06:46","slug":"cluster-linking-06-14-06-46","link":"#cluster-linking-06-14-06-46","children":[]},{"level":3,"title":"Hardware Compatibility 06:46-07:29","slug":"hardware-compatibility-06-46-07-29","link":"#hardware-compatibility-06-46-07-29","children":[]},{"level":3,"title":"Summary 07:29-07:53","slug":"summary-07-29-07-53","link":"#summary-07-29-07-53","children":[]}]},{"level":2,"title":"12.7.8 Clustering Facts","slug":"_12-7-8-clustering-facts","link":"#_12-7-8-clustering-facts","children":[{"level":3,"title":"Hardware Clustering Concepts","slug":"hardware-clustering-concepts","link":"#hardware-clustering-concepts","children":[]},{"level":3,"title":"High Availability Clustering","slug":"high-availability-clustering","link":"#high-availability-clustering","children":[]},{"level":3,"title":"Load Balancing Clustering","slug":"load-balancing-clustering","link":"#load-balancing-clustering","children":[]}]}],"git":{"updatedTime":1736270853000},"filePathRelative":"12/12.07.Redundancy.md"}');export{h as comp,u as data};
